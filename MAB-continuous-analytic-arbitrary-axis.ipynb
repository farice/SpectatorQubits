{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [],
   "source": [
    "# import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "from datetime import timedelta\n",
    "\n",
    "import math\n",
    "import random\n",
    "\n",
    "from qutip import Bloch, Bloch3d, basis\n",
    "from qutip.qip.operations import rz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(axs, frame_idx, rewards, losses, sigma, contextual_bias, mu, elapsed_time):\n",
    "    axs[0].set_title('frame %s. reward: %s. time: %s' % (frame_idx, np.mean(rewards[-10:]), elapsed_time))\n",
    "    axs[0].plot(rewards)\n",
    "    if losses and sigma:\n",
    "        axs[1].set_title('fidelity')\n",
    "        axs[1].plot(losses, 'g')\n",
    "        axs[1].plot(sigma, 'r')\n",
    "    if contextual_bias:\n",
    "        axs[2].set_title('contextual_bias')\n",
    "        axs[2].plot(contextual_bias)\n",
    "        axs[2].axhline(y=mu, color='g', linestyle='-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bloch_viz(b, error_samples, error_draw, contextual_bias, correction):  \n",
    "    b.clear()\n",
    "    error_samples = np.random.choice(error_samples, 100)\n",
    "    kets = [rz(error_sample) * (basis(2, 0) + basis(2, 1)).unit() for error_sample in error_samples]\n",
    "    b.add_states(kets, \"point\")\n",
    "    b.add_states([\n",
    "        rz(error_draw) * (basis(2, 0) + basis(2, 1)).unit(),\n",
    "        rz(-contextual_bias) * (basis(2, 0) + basis(2, 1)).unit(),\n",
    "        rz(-correction) * (basis(2, 0) + basis(2, 1)).unit()\n",
    "        ])\n",
    "    b.point_color = [\"r\"]\n",
    "    b.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (action, reward) distribution for a given \"spectator measurement outcome\" state\n",
    "class Context:\n",
    "    def __init__(self, gamma, eta):\n",
    "        # discount factor\n",
    "        self.gamma = gamma\n",
    "        self.eta = eta\n",
    "\n",
    "        self.batch_correction_feedback = [[], [], []]\n",
    "        self.batch_context_feedback = [[], [], []]\n",
    "        self.context_theta = [0, 0, 0]\n",
    "        self.correction_theta = [0, 0, 0]\n",
    "\n",
    "    def reset(self):\n",
    "        self.batch_correction_feedback = []\n",
    "        self.batch_context_feedback = []\n",
    "\n",
    "    def discount(self):\n",
    "        pass\n",
    "\n",
    "    def update_gamma(self, gamma):\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def update_batch_feedback(self, context_feedback, correction_feedback):\n",
    "        # feedback is given per theta\n",
    "        for idx, f in enumerate(context_feedback):\n",
    "            self.batch_context_feedback[idx].append(f)\n",
    "        \n",
    "        for idx, f in enumerate(correction_feedback):\n",
    "            self.batch_correction_feedback[idx].append(f)\n",
    "\n",
    "    def get_and_combine_optimal_theta(self):\n",
    "        if len(self.batch_correction_feedback) == 0:\n",
    "            return self.correction_theta, self.context_theta\n",
    "\n",
    "        # feedback is given per theta\n",
    "        for idx, f in enumerate(self.batch_correction_feedback):\n",
    "            lo = [r[0] for r in f]\n",
    "            mid = [r[1] for r in f]\n",
    "            hi = [r[2] for r in f]\n",
    "            \n",
    "            mu_plus = np.mean(hi)\n",
    "            mu = np.mean(mid)\n",
    "            mu_minus = np.mean(lo)\n",
    "\n",
    "            var_grad = np.mean([2 * (m - mu)  * (h - l) for l,m,h in zip(lo,mid,hi)])\n",
    "            grad = mu_plus - mu_minus\n",
    "\n",
    "            self.correction_theta[idx] += self.eta * grad\n",
    "        \n",
    "        for idx, f in enumerate(self.batch_context_feedback):\n",
    "            lo = [r[0] for r in f]\n",
    "            mid = [r[1] for r in f]\n",
    "            hi = [r[2] for r in f]\n",
    "            \n",
    "            mu_plus = np.mean(hi)\n",
    "            mu = np.mean(mid)\n",
    "            mu_minus = np.mean(lo)\n",
    "\n",
    "            grad = mu_plus - mu_minus\n",
    "\n",
    "            self.context_theta[idx] += self.eta * var_grad\n",
    "\n",
    "        self.reset()\n",
    "        \n",
    "        return self.correction_theta, self.context_theta\n",
    "\n",
    "    def get_optimal_theta(self):\n",
    "        return self.correction_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# contextual analytic geometric descent\n",
    "class Analytic2D:\n",
    "    def __init__(self, env, initial_gamma=0.99, eta=np.pi/64):\n",
    "        # two contexts -> pos vs neg rotation with respect to a chosen rotational basis\n",
    "        self.contexts = [Context(initial_gamma, eta), Context(initial_gamma, eta)]\n",
    "\n",
    "        self.rewards = []\n",
    "        self.fidelity = []\n",
    "        self.control_fidelity = []\n",
    "\n",
    "        self.num_context_spectators = env.num_context_spectators\n",
    "        self.num_reward_spectators = env.num_reward_spectators\n",
    "\n",
    "        # step size\n",
    "        self.eta = eta\n",
    "\n",
    "    def get_actions(self, observations):\n",
    "        # our context is an array of binary spectator qubit measurements\n",
    "        # hence, we could convert this binary array to an integer and index 2^(spectator qubits) contexts\n",
    "        # context = self.contexts[np.packbits(observation, bitorder='little')[0]] \n",
    "        # for now, we only have two contexts (+ vs -), and so we consider spectators to be indistinguishable noise polling devices\n",
    "        # in the future, we may consider noise gradients and so we do indeed need to track the specific arrangement\n",
    "\n",
    "        actions = []\n",
    "        for observation in observations:\n",
    "            context_idx = 1 if np.sum(observation) > self.num_context_spectators / 2 else 0\n",
    "            context = self.contexts[context_idx]\n",
    "\n",
    "            # (theta vec)\n",
    "            actions.append(context.get_and_combine_optimal_theta())\n",
    "        return actions\n",
    "\n",
    "    def update(self, context_feedback, correction_feedback, observations):\n",
    "        for context in self.contexts:\n",
    "            context.reset()\n",
    "\n",
    "        for _context_feedback, _correction_feedback, observation in zip(context_feedback, correction_feedback, observations):\n",
    "            context_idx = 1 if np.sum(observation) > self.num_context_spectators / 2 else 0\n",
    "            context = self.contexts[context_idx]\n",
    "            context.update_batch_feedback(_context_feedback, _correction_feedback)\n",
    "\n",
    "    def _update_gammas(self, gamma):\n",
    "        for context in self.contexts:\n",
    "            context.update_gamma(gamma)\n",
    "\n",
    "    def save_reward(self, reward):\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    def save_fidelity(self, fidelity):\n",
    "        self.fidelity.append(fidelity)\n",
    "\n",
    "    def save_control_fidelity(self, fidelity):\n",
    "        self.control_fidelity.append(fidelity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 1000\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "MU = np.pi / 16\n",
    "SIGMA = np.pi / 8\n",
    "\n",
    "# add some random harmonics\n",
    "# time_dependent_fn = np.vectorize(lambda x: (2/5) * np.pi * np.sin(2 * np.pi * x / M))\n",
    "error_samples = np.random.normal(MU, SIGMA, M) # + time_dependent_fn(np.arange(M))\n",
    "# error_samples = np.random.choice([-np.pi/4, np.pi / 4], M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spectator_env import SpectatorEnvContinuousV2\n",
    "\n",
    "# describes MDP\n",
    "# - states are given in terms of a #context_spectators bit measurement outcomes\n",
    "# - continuous action space is given by ([-pi, pi], smoothing parameter, contextual_measurement_bias) i.e. \\U(1) \\times U(1) \\times U(1)\n",
    "env = SpectatorEnvContinuousV2(error_samples, batch_size=BATCH_SIZE, num_context_spectators=64, num_reward_spectators=64, context_sensitivity=2.0, reward_sensitivity=2.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3335: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/opt/anaconda3/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot convert float NaN to integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-b5f6010d025f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprev_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_samples_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mprev_observation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeedback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SpectatorQubits/spectator_env.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         self.error_samples_batch = self.error_samples[\n",
      "\u001b[0;32m~/SpectatorQubits/spectator_env.py\u001b[0m in \u001b[0;36m_get_reward\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    417\u001b[0m                 \u001b[0mcorrection_theta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_theta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m                 \u001b[0mpreps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_preps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrection_theta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m                 \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorrection_theta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/SpectatorQubits/spectator_env.py\u001b[0m in \u001b[0;36m_get_preps\u001b[0;34m(self, t)\u001b[0m\n\u001b[1;32m    355\u001b[0m         return [\n\u001b[1;32m    356\u001b[0m             \u001b[0mqeye\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m             \u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m         ]\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/qutip/qobj.py\u001b[0m in \u001b[0;36mexpm\u001b[0;34m(self, method)\u001b[0m\n\u001b[1;32m   1139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'dense'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m             \u001b[0mF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp_expm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'sparse'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/qutip/sparse.py\u001b[0m in \u001b[0;36msp_expm\u001b[0;34m(A, sparse)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtocsc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0mE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspla\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/scipy/sparse/linalg/matfuncs.py\u001b[0m in \u001b[0;36mexpm\u001b[0;34m(A)\u001b[0m\n\u001b[1;32m    593\u001b[0m             [  0.        ,   0.        ,  20.08553692]])\n\u001b[1;32m    594\u001b[0m     \"\"\"\n\u001b[0;32m--> 595\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_expm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_exact_onenorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'auto'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/scipy/sparse/linalg/matfuncs.py\u001b[0m in \u001b[0;36m_expm\u001b[0;34m(A, use_exact_onenorm)\u001b[0m\n\u001b[1;32m    664\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 666\u001b[0;31m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meta_5\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtheta_13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    667\u001b[0m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_ell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m13\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpade13_scaled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot convert float NaN to integer"
     ]
    }
   ],
   "source": [
    "start = timer()\n",
    "\n",
    "md = Analytic2D(env, initial_gamma=1.0, eta=np.pi/8)\n",
    "\n",
    "episode_reward = 0\n",
    "episode_fidelity = []\n",
    "control_fidelity = []\n",
    "\n",
    "MAX_FRAMES = 10 * M // BATCH_SIZE\n",
    "UPDATE_ERROR_SAMPLES_FRAMES = M // BATCH_SIZE\n",
    "\n",
    "PLOT_BLOCH = False\n",
    "PLOT_REWARD = True\n",
    "\n",
    "# \"episodes\" are a reasonable way to think about learning a periodic time dependent function\n",
    "# max frames = episode length * num episodes\n",
    "# episodes are identical sequences of training data\n",
    "observation = env.reset()\n",
    "\n",
    "for frame_idx in range(1, MAX_FRAMES + 1):\n",
    "    actions = md.get_actions(observation)\n",
    "    prev_batch = env.error_samples_batch\n",
    "    prev_observation=observation\n",
    "    observation, feedback, done, info = env.step(actions)\n",
    "    observation = None if done else observation\n",
    "\n",
    "    # generally, in RL we would consider r(s | s', a)\n",
    "    # given a state transition s' -> s due to action a\n",
    "    # for now, we are only interested in r(s', a)\n",
    "    md.update(actions, reward, prev_observation)\n",
    "    episode_reward += np.sum(reward)\n",
    "\n",
    "    for x, y in info:\n",
    "        episode_fidelity.append(x)\n",
    "        control_fidelity.append(y)\n",
    "\n",
    "    if UPDATE_ERROR_SAMPLES_FRAMES:\n",
    "        new_error_samples = np.random.normal(MU, SIGMA, M)\n",
    "        env.set_error_samples(new_error_samples)\n",
    "\n",
    "    if done:\n",
    "        if PLOT_BLOCH or PLOT_REWARD:\n",
    "            clear_output(True)\n",
    "        if PLOT_BLOCH:\n",
    "            bloch_viz(b, env.error_samples, prev_batch[0], md.action_contextual_bias, actions[0][0])\n",
    "        if PLOT_REWARD and len(md.rewards) > 0 and len(md.fidelity) > 0 and len(md.contextual_biases) > 0:\n",
    "            fig, axs = plt.subplots(1, 3, figsize=(20,5))\n",
    "            plot(axs, frame_idx, md.rewards, md.fidelity, md.control_fidelity, md.contextual_biases, -MU,\n",
    "                timedelta(seconds=int(timer()-start)))\n",
    "            plt.show()\n",
    "\n",
    "        observation = env.reset()\n",
    "        md.save_reward(episode_reward)\n",
    "        md.save_fidelity(np.mean(episode_fidelity))\n",
    "        md.save_control_fidelity(np.mean(control_fidelity))\n",
    "        episode_reward = 0\n",
    "        episode_fidelity = []\n",
    "        control_fidelity = []\n",
    "\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.13928068662033852\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(md.contextual_biases))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(md.contexts[0].get_optimal_theta())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.54754075 -0.36648637 -0.71474008]\n"
     ]
    }
   ],
   "source": [
    "print(md.contexts[1].get_optimal_theta())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.21649348 -0.17622204 -0.19074786] 0.19634954084936207\n"
     ]
    }
   ],
   "source": [
    "print(md.action_contextual_bias, MU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
