{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contextual analytic geometric descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODOs\n",
    "\n",
    "* Outer loop plots.\n",
    "* \\>2 contexts, potentially with structure imposed.\n",
    "* Geometric descent (makes gradient updates independent of gate-parameterization).\n",
    "* Add gradient flows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
      "['/Users/farissbahi/Desktop/SpectatorQubits', '/Users/farissbahi/Desktop/SpectatorQubits', '/Users/farissbahi/Desktop/SpectatorQubits', '/Users/farissbahi/Desktop/SpectatorQubits', '/Users/farissbahi/Desktop/SpectatorQubits', '/Users/farissbahi/Desktop/SpectatorQubits', '/Users/farissbahi/Desktop/SpectatorQubits', '/Users/farissbahi/Desktop/SpectatorQubits', '/Users/farissbahi/Desktop/SpectatorQubits', '/Users/farissbahi/Desktop/SpectatorQubits', '/Users/farissbahi/Desktop/SpectatorQubits', '/Users/farissbahi/Desktop/SpectatorQubits', '/Users/farissbahi/Desktop/SpectatorQubits', '/Users/farissbahi/Desktop/SpectatorQubits', '/Users/farissbahi/Desktop/SpectatorQubits', '/Users/farissbahi/Desktop/SpectatorQubits']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ipyparallel as ipp\n",
    "\n",
    "c = ipp.Client()\n",
    "v = c[:]\n",
    "print(c.ids)\n",
    "\n",
    "number_of_engines = 16\n",
    "\n",
    "v.map(os.chdir, ['/Users/farissbahi/Desktop/SpectatorQubits'] * number_of_engines)\n",
    "print(v.apply_sync(os.getcwd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "inputHidden": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "outputHidden": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.5\n",
      "importing functools on engine(s)\n",
      "importing OrderedDict from collections on engine(s)\n",
      "importing numpy on engine(s)\n",
      "importing pandas on engine(s)\n",
      "importing scipy.interpolate on engine(s)\n",
      "importing fmin,tpe,Trials,hp from hyperopt on engine(s)\n",
      "importing default_timer from timeit on engine(s)\n",
      "importing basis,expect from qutip on engine(s)\n",
      "importing rx,rz from qutip.qip.operations on engine(s)\n",
      "importing sigmax from qutip.operators on engine(s)\n",
      "importing snot from qutip.qip.operations on engine(s)\n",
      "importing clear_output from IPython.display on engine(s)\n",
      "importing SpectatorEnvContinuousV2 from spectator_env_v2 on engine(s)\n",
      "importing extract_theta_phi,plot,plot_layered,get_error_unitary from spectator_env_utils_v2 on engine(s)\n",
      "importing ParallelSim from spectator_env_agent_v2 on engine(s)\n"
     ]
    }
   ],
   "source": [
    "import functools\n",
    "from collections import OrderedDict\n",
    "import dill\n",
    "\n",
    "import numpy\n",
    "import pandas\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy.interpolate\n",
    "\n",
    "from hyperopt import fmin, tpe, Trials, hp\n",
    "\n",
    "from timeit import default_timer\n",
    "\n",
    "from qutip import basis, expect\n",
    "from qutip.qip.operations import rx, rz\n",
    "\n",
    "from qutip.operators import sigmax\n",
    "from qutip.qip.operations import snot\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from platform import python_version\n",
    "\n",
    "# Local imports\n",
    "from spectator_env_v2 import SpectatorEnvContinuousV2\n",
    "from spectator_env_utils_v2 import extract_theta_phi, plot, plot_layered, get_error_unitary\n",
    "from spectator_env_agent_v2 import ParallelSim\n",
    "\n",
    "print(python_version())\n",
    "\n",
    "with v.sync_imports():\n",
    "    import functools\n",
    "    from collections import OrderedDict\n",
    "\n",
    "    import numpy\n",
    "    import pandas\n",
    "\n",
    "    import scipy.interpolate\n",
    "\n",
    "    from hyperopt import fmin, tpe, Trials, hp\n",
    "\n",
    "    from timeit import default_timer\n",
    "\n",
    "    from qutip import basis, expect\n",
    "    from qutip.qip.operations import rx, rz\n",
    "\n",
    "    from qutip.operators import sigmax\n",
    "    from qutip.qip.operations import snot\n",
    "\n",
    "    from IPython.display import clear_output\n",
    "\n",
    "    # Local imports\n",
    "    from spectator_env_v2 import SpectatorEnvContinuousV2\n",
    "    from spectator_env_utils_v2 import extract_theta_phi, plot, plot_layered, get_error_unitary\n",
    "    from spectator_env_agent_v2 import ParallelSim\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "p = psutil.Process(os.getpid())\n",
    "# print(p.cpu_affinity())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_loss(meas, trans):\n",
    "    return numpy.var([numpy.pi - numpy.arccos(\n",
    "        numpy.sqrt(t.overlap(meas) * meas.overlap(t))) for t in trans])\n",
    "\n",
    "\n",
    "def correction_loss(meas, trans):\n",
    "    return numpy.mean([numpy.pi - numpy.arccos(\n",
    "        numpy.sqrt(t.overlap(meas) * meas.overlap(t))) for t in trans])\n",
    "\n",
    "\n",
    "# This is the contour of the \"true loss\" for correction and contextualization.\n",
    "def get_contour(error_samples, loss_fn, sensitivity=1.0):\n",
    "    prepared_basis = [snot() * basis(2, 0), snot() * sigmax() * basis(2, 0)]\n",
    "    trans = [get_error_unitary(sample, sensitivity) * prepared_basis[0]\n",
    "             for sample in error_samples]\n",
    "\n",
    "    thetas = numpy.linspace(0, numpy.pi, 33)\n",
    "    phis = numpy.linspace(-numpy.pi, numpy.pi, 33)\n",
    "    loss = numpy.zeros((len(thetas), len(phis)))\n",
    "\n",
    "    min_phi = 0.\n",
    "    min_theta = 0.\n",
    "    min_loss = 1.\n",
    "\n",
    "    max_phi = 0.\n",
    "    max_theta = 0.\n",
    "    max_loss = 0.\n",
    "    for i, theta in enumerate(thetas):\n",
    "        for j, phi in enumerate(phis):\n",
    "            meas = numpy.cos(theta / 2) * prepared_basis[0] + numpy.exp(\n",
    "                1j * phi) * numpy.sin(theta / 2) * prepared_basis[1]\n",
    "            meas = meas.unit()\n",
    "\n",
    "            objective = loss_fn(meas, trans)\n",
    "            if (numpy.abs(objective) < min_loss):\n",
    "                min_loss = objective\n",
    "                min_phi = phi\n",
    "                min_theta = theta\n",
    "            if (numpy.abs(objective) > max_loss):\n",
    "                max_loss = objective\n",
    "                max_phi = phi\n",
    "                max_theta = theta\n",
    "            loss[i][j] = numpy.abs(objective)\n",
    "    return {'thetas': thetas, 'phis': phis, 'loss': loss,\n",
    "            'max_phi': max_phi, 'max_theta': max_theta,\n",
    "            'min_phi': min_phi, 'min_theta': min_theta}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Hyperparameters\n",
    "'''\n",
    "\n",
    "# Error distribution.\n",
    "MU_1 = 0\n",
    "SIGMA_1 = numpy.pi / 4\n",
    "MU_2 = 0\n",
    "SIGMA_2 = numpy.pi / 32\n",
    "MU_3 = 0\n",
    "SIGMA_3 = numpy.pi / 64\n",
    "\n",
    "# Length of bit-string used to condition error distribution.\n",
    "NUM_CONTEXT_SPECTATORS = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sim(parallel_sims_store, context_eta, correction_eta, context_eta_init,\n",
    "            correction_eta_init_1, correction_eta_init_2,\n",
    "            batch_size, num_context_spectators,\n",
    "            error_samples_generator_partial,\n",
    "            feedback_spectators_allocation_function,\n",
    "            num_batches_to_combine_context_feedback,\n",
    "            num_batches_to_combine_correction_feedback,\n",
    "            num_epochs, burnin_length):\n",
    "    parallel_sim = ParallelSim(\n",
    "                context_eta=context_eta, correction_eta=correction_eta,\n",
    "                context_eta_init=context_eta_init,\n",
    "                correction_eta_init=[correction_eta_init_1,\n",
    "                                     correction_eta_init_2],\n",
    "                batch_size=batch_size,\n",
    "                num_context_spectators=num_context_spectators,\n",
    "                error_samples_generator=error_samples_generator_partial,\n",
    "                feedback_spectators_allocation_function=feedback_spectators_allocation_function,\n",
    "                # How we vary batch size for context vs. correction feedback gradients.\n",
    "                num_batches_to_combine_context_feedback=num_batches_to_combine_context_feedback,\n",
    "                num_batches_to_combine_correction_feedback=num_batches_to_combine_correction_feedback)\n",
    "    all_done = False\n",
    "    # results = {p: [] for p in parallel_sims}\n",
    "    # results can be polled for specific hyperparams\n",
    "    for epoch in range(num_epochs):\n",
    "        while not all_done:\n",
    "            r = parallel_sim.step()\n",
    "            all_done = r.done\n",
    "        all_done = False\n",
    "        if epoch % 50 == 0:\n",
    "            print(f\"Completed epoch {epoch}\")\n",
    "    parallel_sim.env.close()\n",
    "    data_fids = numpy.array(parallel_sim.data_fidelity_per_episode[burnin_length:])\n",
    "    ctrl_fids = numpy.array(parallel_sim.control_fidelity_per_episode[burnin_length:])\n",
    "    loss = -numpy.mean(data_fids - ctrl_fids, axis=0)\n",
    "    parallel_sims_store[loss] = parallel_sim\n",
    "    parallel_sim.destruct()\n",
    "    print(f\"Loss: {loss}\")\n",
    "    return loss\n",
    "\n",
    "\n",
    "def objective(args, parallel_sims_store, run_sim,\n",
    "              error_samples_generator,\n",
    "              feedback_spectators_allocation_function, mu, sigma,\n",
    "              num_context_spectators, drift_strength, resource_regime):\n",
    "    print(f\"Args: {args}\")\n",
    "    context_eta_init = [args['context_theta_init_0'],\n",
    "                        args['context_theta_init_1'],\n",
    "                        args['context_theta_init_2']]\n",
    "    correction_eta_init_1 = [args['correction_theta_init_1_0'],\n",
    "                             args['correction_theta_init_1_1'],\n",
    "                             args['correction_theta_init_1_2']]\n",
    "    correction_eta_init_2 = [args['correction_theta_init_2_0'],\n",
    "                             args['correction_theta_init_2_1'],\n",
    "                             args['correction_theta_init_2_2']]\n",
    "    # DO NOT MODIFY - set batch size via num_batches_to_combine...\n",
    "    batch_size = 1 \n",
    "    error_samples_generator_partial = functools.partial(\n",
    "        error_samples_generator, mu=mu, sigma=sigma,\n",
    "        batch_size=batch_size, drift=numpy.pi / drift_strength)\n",
    "    num_epochs = int(numpy.ceil(1000 / batch_size))\n",
    "    burnin_length = min(int(numpy.ceil(800 / batch_size)) - 1,\n",
    "                        int(numpy.ceil(1000 / batch_size) -\n",
    "                            numpy.ceil(2 * drift_strength / batch_size)))\n",
    "    print(F\"epochs: {num_epochs} burnin_length: {burnin_length} batch_size: {batch_size}\")\n",
    "    feedback_alloc = functools.partial(feedback_spectators_allocation_function,\n",
    "                                       resource_regime)\n",
    "    return run_sim(parallel_sims_store=parallel_sims_store,\n",
    "                   context_eta_init=context_eta_init,\n",
    "                   correction_eta_init_1=correction_eta_init_1,\n",
    "                   correction_eta_init_2=correction_eta_init_2,\n",
    "                   num_epochs=num_epochs,\n",
    "                   burnin_length=burnin_length,\n",
    "                   batch_size=batch_size,\n",
    "                   correction_eta=args['correction_eta'],\n",
    "                   context_eta=args['context_eta'],\n",
    "                   error_samples_generator_partial=error_samples_generator_partial,\n",
    "                   num_batches_to_combine_context_feedback=args['num_batches_to_combine_context_feedback'],\n",
    "                   num_batches_to_combine_correction_feedback=args['num_batches_to_combine_correction_feedback'],\n",
    "                   num_context_spectators=num_context_spectators,\n",
    "                   feedback_spectators_allocation_function=feedback_alloc)\n",
    "\n",
    "\n",
    "# minimize the objective over the space\n",
    "# There are 4 critical params to tune => min 2**4 trials reasonably needed.\n",
    "def find_fmin(drift_strength, resource_regime, objective, max_evals=2**7):\n",
    "    start = default_timer()\n",
    "    parallel_sims_store = OrderedDict()\n",
    "    # define a search space\n",
    "    space = OrderedDict([('context_theta_init_0',\n",
    "                          hp.uniform('context_theta_init_0',\n",
    "                                     -numpy.pi, numpy.pi)),\n",
    "                         ('context_theta_init_1',\n",
    "                          hp.uniform('context_theta_init_1',\n",
    "                                     -numpy.pi, numpy.pi)),\n",
    "                         ('context_theta_init_2',\n",
    "                          hp.uniform('context_theta_init_2',\n",
    "                                     -numpy.pi, numpy.pi)),\n",
    "                         ('correction_theta_init_1_0',\n",
    "                          hp.uniform('correction_theta_init_1_0',\n",
    "                                     -numpy.pi, numpy.pi)),\n",
    "                         ('correction_theta_init_1_1',\n",
    "                          hp.uniform('correction_theta_init_1_1',\n",
    "                                     -numpy.pi, numpy.pi)),\n",
    "                         ('correction_theta_init_1_2',\n",
    "                          hp.uniform('correction_theta_init_1_2',\n",
    "                                     -numpy.pi, numpy.pi)),\n",
    "                         ('correction_theta_init_2_0',\n",
    "                          hp.uniform('correction_theta_init_2_0',\n",
    "                                     -numpy.pi, numpy.pi)),\n",
    "                         ('correction_theta_init_2_1',\n",
    "                          hp.uniform('correction_theta_init_2_1',\n",
    "                                     -numpy.pi, numpy.pi)),\n",
    "                         ('correction_theta_init_2_2',\n",
    "                          hp.uniform('correction_theta_init_2_2',\n",
    "                                     -numpy.pi, numpy.pi)),\n",
    "                         ('num_batches_to_combine_correction_feedback',\n",
    "                          hp.quniform('num_batches_to_combine_correction_feedback',\n",
    "                                      1, 16, 1)),\n",
    "                         ('num_batches_to_combine_context_feedback',\n",
    "                          hp.quniform('num_batches_to_combine_context_feedback',\n",
    "                                      1, 16, 1)),\n",
    "                         ('correction_eta',\n",
    "                          hp.loguniform('correction_eta',\n",
    "                                        numpy.log(numpy.pi/256),\n",
    "                                        numpy.log(numpy.pi / 4))),\n",
    "                         ('context_eta',\n",
    "                          hp.loguniform('context_eta',\n",
    "                                        numpy.log(numpy.pi/256),\n",
    "                                        numpy.log(numpy.pi / 4)))])\n",
    "    objective_partial = functools.partial(objective,\n",
    "                                          parallel_sims_store=parallel_sims_store,\n",
    "                                          drift_strength=drift_strength,\n",
    "                                          resource_regime=resource_regime)\n",
    "    fmin(objective_partial, space=space, algo=tpe.suggest, max_evals=max_evals)\n",
    "    print(\"Total runtime: \", default_timer()-start)\n",
    "    return parallel_sims_store\n",
    "\n",
    "\n",
    "# Spectators can used for two types of feedback:\n",
    "# (1) Improving the contextual conditioning.\n",
    "# (2) Improving the correction given each contextual conditioning.\n",
    "def feedback_spectators_allocation_function(multiplier=1):\n",
    "    feedback_alloc = {}\n",
    "    feedback_alloc['correction'] = 4 * multiplier\n",
    "    feedback_alloc['context'] = 6 * multiplier\n",
    "#     feedback_alloc['correction'] = NUM_REWARD_SPECTATORS / 2\n",
    "#     feedback_alloc['context'] = (NUM_REWARD_SPECTATORS\n",
    "#                                  - feedback_alloc['correction'])\n",
    "    return feedback_alloc\n",
    "\n",
    "\n",
    "def error_samples_generator(mu, sigma, drift, iteration, batch_size):\n",
    "    # return numpy.random.choice([mu - sigma, mu + sigma], m)\n",
    "    # return list(zip(numpy.random.uniform(mu[0] - sigma[0], mu[0] + sigma[0], m),\n",
    "    #                 numpy.random.uniform(mu[1] - sigma[1], mu[1] + sigma[1], m)))\n",
    "    # return numpy.random.normal(MU, SIGMA, M) # + time_dependent_fn(numpy.arange(M))\n",
    "    direction = numpy.array([numpy.sqrt(98)/10, 1/10, 1/10])\n",
    "\n",
    "    return list(zip([numpy.random.normal(mu[0] + (iteration + i) * direction[0] * drift, sigma[0]) for i in range(batch_size)],\n",
    "                    [numpy.random.normal(mu[1] + (iteration + i) * direction[1] * drift, sigma[1]) for i in range(batch_size)],\n",
    "                    [numpy.random.normal(mu[2] + (iteration + i) * direction[2] * drift, sigma[2]) for i in range(batch_size)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "drift_strengths = numpy.repeat([2 ** 10, 2 ** 8, 2**6, 2**4], 4)\n",
    "resource_regime = numpy.tile([2 ** 6, 2 ** 4, 2 ** 2, 2 ** 0], 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_partial = functools.partial(objective,\n",
    "                                      run_sim=run_sim,\n",
    "                                      error_samples_generator=error_samples_generator,\n",
    "                                      feedback_spectators_allocation_function=feedback_spectators_allocation_function,\n",
    "                                      mu=[MU_1, MU_2, MU_3],da\n",
    "                                      sigma=[SIGMA_1, SIGMA_2, SIGMA_3],\n",
    "                                      num_context_spectators=NUM_CONTEXT_SPECTATORS)\n",
    "find_fmin_partial = functools.partial(find_fmin, objective=objective_partial, max_evals=2**10)\n",
    "parallel_sim_stores_all = v.map(find_fmin_partial, drift_strengths, resource_regime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # initialize a stdout0 array for comparison\n",
    "# stdout0 = parallel_sim_stores_all.stdout\n",
    "\n",
    "# while not parallel_sim_stores_all.ready():\n",
    "#     # check if stdout changed for any kernel\n",
    "#     if parallel_sim_stores_all.stdout != stdout0:\n",
    "#         for i in range(0,len(parallel_sim_stores_all.stdout)):\n",
    "#             if parallel_sim_stores_all.stdout[i] != stdout0[i]:\n",
    "#                 # print only new stdout's without previous message and remove '\\n' at the end\n",
    "#                 print('kernel ' + str(i) + ': ' + parallel_sim_stores_all.stdout[i][len(stdout0[i]):-1])\n",
    "\n",
    "#                 # set stdout0 to last output for new comparison\n",
    "#                 stdout0 =  parallel_sim_stores_all.stdout\n",
    "#     else:\n",
    "#         continue\n",
    "\n",
    "\n",
    "parallel_sim_stores_all.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel_sim_stores_all.display_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dill.dump_session('notebook_env.db')\n",
    "# with open('parallel_sim_stores_all_2021_01_29.pkl', 'wb') as f:\n",
    "#     dill.dump(parallel_sim_stores_all.get(), f)\n",
    "with open('parallel_sim_stores_all_2021_01_29.pkl', 'rb') as f:\n",
    "    parallel_sim_stores_all_2 = dill.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted(parallel_sim_stores_all.get()[3].keys(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 15\n",
    "drift_strength = drift_strengths[idx]\n",
    "parallel_sims_store_red = [p[1] for p in sorted(parallel_sim_stores_all[idx].items())][:2]\n",
    "\n",
    "batch_size = 1\n",
    "burnin_length = min(int(numpy.ceil(800 / batch_size)) - 1,\n",
    "                        int(numpy.ceil(1000 / batch_size) - numpy.ceil(2 * drift_strength / batch_size)))\n",
    "# Get contour over entire distribution. May need to subset for non-stationary case.\n",
    "error_samples_generator_partial = functools.partial(\n",
    "        error_samples_generator, mu=[MU_1, MU_2, MU_3], sigma=[SIGMA_1, SIGMA_2, SIGMA_3], batch_size=2 * drift_strength, drift=numpy.pi / drift_strength)\n",
    "context_contour = get_contour(error_samples_generator_partial(iteration=0), context_loss)\n",
    "optimal_phi, optimal_theta = context_contour['max_phi'], context_contour['max_theta']\n",
    "if optimal_phi > numpy.pi:\n",
    "    optimal_phi += numpy.pi\n",
    "\n",
    "\n",
    "# Allows plotting of conditioned correction contours according to the true\n",
    "# optimal conditioning. Previously, we conditioned based upon the current\n",
    "# estimator for conditioning.\n",
    "def condition_error_samples(error_samples, optimal_phi, optimal_theta):\n",
    "    polling_group = []\n",
    "    prepared_basis = [snot() * basis(2, 0), snot() * sigmax() * basis(2, 0)]\n",
    "    for e in error_samples:\n",
    "        u = get_error_unitary(e, sensitivity=1.0)\n",
    "        s = u * prepared_basis[0]\n",
    "        meas = numpy.cos(optimal_theta / 2) * prepared_basis[0] + numpy.exp(\n",
    "                1j * optimal_phi) * numpy.sin(optimal_theta / 2) * prepared_basis[1]\n",
    "        meas = meas.unit()\n",
    "        obj = numpy.real(meas.overlap(s) * s.overlap(meas))\n",
    "        polling_group.append(obj > 0.5)\n",
    "    return polling_group\n",
    "\n",
    "\n",
    "error_samples = numpy.array(error_samples_generator_partial(iteration=0))\n",
    "cond = numpy.array(condition_error_samples(error_samples, optimal_phi, optimal_theta))\n",
    "correction_contour = {}\n",
    "correction_contour[0] = get_contour(error_samples[\n",
    "    cond], correction_loss)\n",
    "correction_contour[1] = get_contour(error_samples[\n",
    "    ~cond], correction_loss)\n",
    "plot_layered(parallel_sims_store_red, context_contour, correction_contour, burnin_length=burnin_length, window_size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## burnin_length = 10\n",
    "# d = {\n",
    "#     \"batch_number\": numpy.array([]),\n",
    "#     \"context_eta_init\": numpy.array([]),\n",
    "#     \"correction_eta_init_0\": numpy.array([]),\n",
    "#     \"correction_eta_init_1\": numpy.array([]),\n",
    "#     \"relative_fid\": numpy.array([])\n",
    "# }\n",
    "# for idx, sim in enumerate(parallel_sims):\n",
    "#         data_fids = numpy.mean(numpy.array([s.data_fidelity_per_episode[burnin_length:] for s in sim]), axis=0)\n",
    "#         ctrl_fids = numpy.mean(numpy.array([s.control_fidelity_per_episode[burnin_length:] for s in sim]), axis=0)\n",
    "\n",
    "#         rel_fids = data_fids / ctrl_fids\n",
    "#         for i, r in enumerate(rel_fids):\n",
    "#             d[\"batch_number\"] = numpy.append(d[\"batch_number\"], i)\n",
    "#             d[\"relative_fid\"] = numpy.append(d[\"relative_fid\"], r)\n",
    "#             d[\"context_eta_init\"] = numpy.append(d[\"context_eta_init\"], sim[0].context_eta_init[0])\n",
    "#             d[\"correction_eta_init_0\"] = numpy.append(d[\"correction_eta_init_0\"], sim[0].correction_eta_init[0][0])\n",
    "#             d[\"correction_eta_init_1\"] = numpy.append(d[\"correction_eta_init_1\"], sim[0].correction_eta_init[1][0])\n",
    "\n",
    "\n",
    "# df = pd.DataFrame.from_dict(d)\n",
    "# g = sns.relplot(\n",
    "#     data=df,\n",
    "#     x=\"context_eta_init\", y=\"correction_eta_init_0\", hue=\"relative_fid\", size=\"relative_fid\"\n",
    "# )\n",
    "\n",
    "# # Tweak the figure to finalize\n",
    "# g.set(xlabel=\"\", ylabel=\"\", aspect=\"equal\")\n",
    "# g.despine(left=True, bottom=True)\n",
    "# g.ax.margins(.02)\n",
    "# for label in g.ax.get_xticklabels():\n",
    "#     label.set_rotation(90)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix breakdowns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def analytic_context_grad(meas_lo, meas_mid, meas_hi, trans):\n",
    "#     lo = numpy.array(\n",
    "#         [numpy.sqrt(t.overlap(meas_lo) * meas_lo.overlap(t)) for t in trans])\n",
    "#     mid = numpy.array(\n",
    "#         [numpy.sqrt(t.overlap(meas_mid) * meas_mid.overlap(t)) for t in trans])\n",
    "#     hi = numpy.array(\n",
    "#         [numpy.sqrt(t.overlap(meas_hi) * meas_hi.overlap(t)) for t in trans])\n",
    "\n",
    "#     mean_mid = numpy.mean(mid)\n",
    "#     mean_lo = numpy.mean(lo)\n",
    "#     mean_hi = numpy.mean(hi)\n",
    "#     var_grad = numpy.mean([2 * (m - mean_mid) * ((h - l) - (mean_hi - mean_lo))\n",
    "#                         for l, m, h in zip(lo, mid, hi)])\n",
    "#     return var_grad\n",
    "\n",
    "\n",
    "# # This is the contour of the *gradient* we actually compute using\n",
    "# # parameter-shift-measurable observables. Spectator qubits achieve a finite\n",
    "# # sampling of this expectation value.\n",
    "# def get_analytic_contour(error_samples, num_variational_params=11):\n",
    "#     prepared_basis = [snot() * basis(2, 0), snot() * sigmax() * basis(2, 0)]\n",
    "#     trans = [rz(sample) * prepared_basis[0] for sample in error_samples]\n",
    "\n",
    "#     variational_params = numpy.linspace(-numpy.pi, numpy.pi - (numpy.pi/128),\n",
    "#                                      num_variational_params)\n",
    "#     theta_phi_feedback = {}\n",
    "\n",
    "#     for t_1 in variational_params:\n",
    "#         for t_2 in variational_params:\n",
    "#             for t_3 in variational_params:\n",
    "#                 t = [t_1, t_2, t_3]\n",
    "#                 unitary_mid = (rz(t[2]) * rx(-numpy.pi/2) * rz(t[1]) *\n",
    "#                                rx(numpy.pi / 2) * rz(t[0]))\n",
    "\n",
    "#                 theta_mid, phi_mid = extract_theta_phi(unitary_mid.dag())\n",
    "#                 meas_mid = (numpy.cos(theta_mid / 2) * prepared_basis[0] + numpy.exp(\n",
    "#                     1j * phi_mid) * numpy.sin(theta_mid / 2) * prepared_basis[1])\n",
    "#                 meas_mid = meas_mid.unit()\n",
    "\n",
    "#                 if (theta_mid, phi_mid) in theta_phi_feedback.keys():\n",
    "#                     continue\n",
    "\n",
    "#                 g = []\n",
    "#                 for i in range(3):\n",
    "#                     t[i] += -numpy.pi / 2\n",
    "#                     unitary_lo = (rz(t[2]) * rx(-numpy.pi/2) * rz(t[1]) *\n",
    "#                                   rx(numpy.pi / 2) * rz(t[0]))\n",
    "#                     t[i] += numpy.pi\n",
    "#                     unitary_hi = (rz(t[2]) * rx(-numpy.pi/2) * rz(t[1]) *\n",
    "#                                   rx(numpy.pi / 2) * rz(t[0]))\n",
    "#                     t[i] += -numpy.pi / 2\n",
    "\n",
    "#                     theta_lo, phi_lo = extract_theta_phi(unitary_lo.dag())\n",
    "#                     theta_hi, phi_hi = extract_theta_phi(unitary_hi.dag())\n",
    "\n",
    "#                     meas_lo = (numpy.cos(theta_lo / 2) * prepared_basis[0]\n",
    "#                                + numpy.exp(1j * phi_lo) * numpy.sin(theta_lo / 2)\n",
    "#                                * prepared_basis[1]).unit()\n",
    "#                     meas_hi = (numpy.cos(theta_hi / 2) * prepared_basis[0]\n",
    "#                                + numpy.exp(1j * phi_hi) * numpy.sin(theta_hi / 2)\n",
    "#                                * prepared_basis[1]).unit()\n",
    "\n",
    "#                     g.append(analytic_context_grad(meas_lo=meas_lo,\n",
    "#                                                    meas_mid=meas_mid,\n",
    "#                                                    meas_hi=meas_hi,\n",
    "#                                                    trans=trans))\n",
    "\n",
    "#                 theta_phi_feedback[(round(theta_mid, 2),\n",
    "#                                     round(phi_mid, 2))] = numpy.linalg.norm(g)\n",
    "\n",
    "#     theta_grid = []\n",
    "#     phi_grid = []\n",
    "#     feedback = []\n",
    "\n",
    "#     for (t, p), f in theta_phi_feedback.items():\n",
    "#         theta_grid.append(t)\n",
    "#         phi_grid.append(p)\n",
    "#         feedback.append(f)\n",
    "#     theta_grid = numpy.array(theta_grid)\n",
    "#     phi_grid = numpy.array(phi_grid)\n",
    "#     mesh_theta = mesh_phi = numpy.linspace(-numpy.pi, numpy.pi,\n",
    "#                                         len(variational_params ** 3))\n",
    "#     mesh_theta, mesh_phi = numpy.meshgrid(mesh_theta, mesh_phi)\n",
    "\n",
    "#     # Interpolate using radial basis kernel.\n",
    "#     rbf = scipy.interpolate.Rbf(theta_grid, phi_grid, feedback,\n",
    "#                                 function='linear')\n",
    "#     loss = rbf(mesh_theta, mesh_phi)\n",
    "\n",
    "#     return {'mesh_phi': mesh_phi, 'mesh_theta': mesh_theta, 'loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # This is very slow. O(n^3) computation, for n=num_variational_params.\n",
    "# context_analytic_contour = get_analytic_contour(ERROR_SAMPLES,\n",
    "#                                                 num_variational_params=15)\n",
    "# print(len(context_analytic_contour['mesh_phi']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f, a = plt.subplots(1, 1, figsize=(7.5, 7.5), subplot_kw=dict(polar=True))\n",
    "# CS = a.contourf(context_analytic_contour['mesh_phi'],\n",
    "#                 context_analytic_contour['mesh_theta'],\n",
    "#                 context_analytic_contour['loss'])\n",
    "# a.scatter(x=[numpy.pi/2], y=[numpy.pi/2], label='ideal')\n",
    "# # a.clabel(CS, inline=True, fontsize=14, colors='r',\n",
    "# #          manual=[(-numpy.pi/2, numpy.pi/2)])\n",
    "# a.legend()\n",
    "# a.set_title(\"True analytic context gradient magnitude\")\n",
    "\n",
    "# cbar = f.colorbar(CS)\n",
    "# f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-2-3-gpu.2-3.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-2-3-gpu.2-3:m59"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
